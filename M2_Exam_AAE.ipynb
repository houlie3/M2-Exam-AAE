{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# M2 Written Exam\n",
        "\n",
        "### Group: AAE\n",
        "\n",
        "\n",
        "*   amogha25@student.aau.dk\n",
        "*   alang22@student.aau.dk\n",
        "* egraca22@student.aau.dk\n",
        "\n",
        "Choosen dataset: S&P 500 earnings call transcripts\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GrcUkoUrE77i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this cell, we install all required Python libraries (pandas, networkx, seaborn, spacy, etc.), import the necessary modules, and configure the Gemini API using our stored API key. This prepares the environment for data processing, LLM extraction, and network analysis in the following steps."
      ],
      "metadata": {
        "id": "qoJwkNDK9_CF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "C19RlNHJBOQB",
        "outputId": "8542120a-ebb1-4d06-e220-b68cc17fd21c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (3.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.8)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.8)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.2)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.20.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.11.10)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.10.5)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (8.3.0)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SecretNotFoundError",
          "evalue": "Secret GEMINI_API_KEY does not exist.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSecretNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-608852610.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mapi_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GEMINI_API_KEY\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mapi_key\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"‚ö†Ô∏è Set GEMINI_API_KEY in Colab secrets\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/userdata.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'exists'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mSecretNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'access'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mNotebookAccessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSecretNotFoundError\u001b[0m: Secret GEMINI_API_KEY does not exist."
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "!pip install pandas networkx matplotlib seaborn datasets tqdm spacy\n",
        "\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datasets import load_dataset\n",
        "import time\n",
        "\n",
        "# Gemini API setup\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "api_key = userdata.get(\"GEMINI_API_KEY\")\n",
        "if not api_key:\n",
        "    raise ValueError(\"‚ö†Ô∏è Set GEMINI_API_KEY in Colab secrets\")\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key=api_key,\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this cell, we load the S&P 500 earnings call transcript dataset, shuffle it with a fixed seed to ensure reproducibility, and select a small subset of 20 transcripts for analysis. We also print the company ticker symbols to verify that the sampling is random."
      ],
      "metadata": {
        "id": "wHvV8zYi-HX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"kurry/sp500_earnings_transcripts\", split=\"train\")\n",
        "# Shuffle the dataset with a fixed seed for reproducibility\n",
        "shuffled_dataset = dataset.shuffle(seed=42)\n",
        "\n",
        "# Select 20 random entries and convert to mutable list of dicts\n",
        "subset = [dict(item) for item in shuffled_dataset.select(range(20))]\n",
        "\n",
        "# Check the symbols to confirm randomness\n",
        "for i, item in enumerate(subset):\n",
        "    print(i, item['symbol'])\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "XLxMyRdyBS3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)"
      ],
      "metadata": {
        "id": "z8kh5RoWLrkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this cell, we use the spaCy NLP model to scan each transcript and extract only the sentences that contain predefined risk-related keywords (e.g., ‚Äúrisk‚Äù, ‚Äúcompliance‚Äù, ‚Äúfraud‚Äù, ‚Äúbreach‚Äù, etc.). We store these filtered sentences in the dataset as filtered_text, reducing the transcript length and keeping only the most relevant content for the LLM extraction step."
      ],
      "metadata": {
        "id": "nHGrgZ_L-bel"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Define your risk-related keywords\n",
        "risk_keywords = {\n",
        "    # General risk\n",
        "    \"risk\", \"hazard\", \"danger\", \"threat\", \"incident\", \"accident\",\n",
        "    \"exposure\", \"harm\", \"injury\", \"loss\", \"failure\", \"vulnerability\",\n",
        "    \"unsafe\", \"emergency\", \"disaster\", \"mitigation\", \"prevention\",\n",
        "\n",
        "    # Workplace & safety\n",
        "    \"safety\", \"violation\", \"compliance\", \"inspection\", \"protocol\",\n",
        "    \"control\", \"equipment\", \"protective\", \"training\", \"audit\",\n",
        "    \"hazardous\", \"containment\",\n",
        "\n",
        "    # Financial\n",
        "    \"liability\", \"insurance\", \"fraud\", \"default\", \"volatility\",\n",
        "    \"uncertainty\", \"instability\", \"debt\", \"bankruptcy\",\n",
        "\n",
        "    # Cybersecurity\n",
        "    \"breach\", \"malware\", \"attack\", \"compromise\", \"intrusion\",\n",
        "    \"unauthorized\", \"encryption\", \"security\", \"ransomware\",\n",
        "\n",
        "    # Health & environmental\n",
        "    \"toxic\", \"contamination\", \"pollution\", \"infection\", \"disease\",\n",
        "    \"chemical\", \"spill\", \"evacuation\", \"outbreak\"\n",
        "}\n",
        "\n",
        "# Convert to lowercase for case-insensitive matching\n",
        "risk_keywords = set([kw.lower() for kw in risk_keywords])\n",
        "\n",
        "for i, item in enumerate(subset):\n",
        "    text = item[\"content\"][:5000]\n",
        "    doc = nlp(text)\n",
        "\n",
        "    risk_sentences = [\n",
        "        sent.text.strip()\n",
        "        for sent in doc.sents\n",
        "        if any(kw in sent.text.lower() for kw in risk_keywords)\n",
        "    ]\n",
        "\n",
        "    filtered_text = \" \".join(risk_sentences)\n",
        "\n",
        "    # Save it into the SAME subset\n",
        "    item[\"filtered_text\"] = filtered_text\n",
        "\n",
        "    char_len = len(filtered_text)\n",
        "    word_len = len(filtered_text.split())\n",
        "\n",
        "    print(f\"Transcript {i}: {char_len} characters, {word_len} words (risk-related only)\")\n",
        "    print(f\"‚Üí Sentences kept: {len(risk_sentences)}\")\n",
        "    print(\"-\" * 80)"
      ],
      "metadata": {
        "id": "jy9YYGfcRzNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell prints the character length of the filtered_text for each transcript, allowing us to confirm that the spaCy filtering step successfully extracted risk-related sentences and that no entries are empty."
      ],
      "metadata": {
        "id": "Tscob68q-kW1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, item in enumerate(subset):\n",
        "    print(f\"{i}: filtered_text length={len(item.get('filtered_text',''))}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "IhSkyTDEsM3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell defines the complete pipeline for extracting structured risk information with Gemini.\n",
        "It includes:\n",
        "\n",
        "* clean_json(): Safely extracts a JSON array from the model‚Äôs output.\n",
        "\n",
        "* call_gemini_with_retry(): Sends prompts to Gemini with automatic retries if the model is overloaded.\n",
        "\n",
        "* extract_risks_gemini_full_text(): Sends each transcript‚Äôs filtered risk-related text to Gemini and receives structured risk objects (company, risk_type, sentence).\n",
        "\n",
        "* process_transcripts_filtered(): Loops through all transcripts, runs extraction, and aggregates all detected risks.\n",
        "\n",
        "This is the core step where the LLM converts unstructured text into structured data for further analysis."
      ],
      "metadata": {
        "id": "I_6-eNU9_B4p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- JSON cleaning ---\n",
        "def clean_json(text):\n",
        "    \"\"\"Extracts the first JSON array from LLM output, ignoring extra text.\"\"\"\n",
        "    match = re.search(r\"\\[.*\\]\", text, flags=re.DOTALL)\n",
        "    if match:\n",
        "        try:\n",
        "            return json.loads(match.group(0))\n",
        "        except json.JSONDecodeError:\n",
        "            return []\n",
        "    return []\n",
        "\n",
        "# --- Gemini call with retry ---\n",
        "def call_gemini_with_retry(prompt, model=\"gemini-2.5-flash\", max_retries=5, wait_time=5):\n",
        "    \"\"\"Call Gemini API safely with retries on overload.\"\"\"\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = client.chat.completions.create(\n",
        "                model=model,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                temperature=0\n",
        "            )\n",
        "            return response.choices[0].message.content\n",
        "        except Exception as e:\n",
        "            err_msg = str(e)\n",
        "            if \"503\" in err_msg or \"UNAVAILABLE\" in err_msg:\n",
        "                print(f\"‚ö†Ô∏è Model overloaded, retrying in {wait_time}s... (attempt {attempt+1}/{max_retries})\")\n",
        "                time.sleep(wait_time)\n",
        "                wait_time *= 2\n",
        "            else:\n",
        "                print(\"‚ùå Unexpected error:\", e)\n",
        "                return None\n",
        "    print(\"‚ùå Max retries exceeded.\")\n",
        "    return None\n",
        "\n",
        "# --- Risk extraction without further chunking ---\n",
        "def extract_risks_gemini_full_text(filtered_text, company_name, model=\"gemini-2.5-flash\"):\n",
        "    \"\"\"\n",
        "    Sends the already filtered transcript to Gemini without splitting into chunks.\n",
        "    Returns a list of dictionaries.\n",
        "    \"\"\"\n",
        "    all_risks = []\n",
        "\n",
        "    if not filtered_text.strip():\n",
        "        return all_risks  # skip empty filtered text\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Extract all risks mentioned in the following earnings transcript.\n",
        "    For each risk, provide a JSON array where each element is an object with keys:\n",
        "    \"company\": \"{company_name}\",\n",
        "    \"risk_type\": type of risk discussed (e.g., market, operational, regulatory, legal, supply chain),\n",
        "    \"sentence\": the exact sentence mentioning it.\n",
        "    Your entire response must be a valid JSON array, with no extra text.\n",
        "\n",
        "    Text: {filtered_text}\n",
        "    \"\"\"\n",
        "    content = call_gemini_with_retry(prompt, model=model)\n",
        "    if content:\n",
        "        risks_chunk = clean_json(content)\n",
        "        all_risks.extend(risks_chunk)\n",
        "\n",
        "    return all_risks\n",
        "\n",
        "# --- Main Loop ---\n",
        "def process_transcripts_filtered(subset, model=\"gemini-2.5-flash\", output_path=\"all_risks_extracted.json\"):\n",
        "    all_risks = []\n",
        "\n",
        "    for i, item in enumerate(tqdm(subset, desc=\"Processing transcripts\")):\n",
        "        company = item.get(\"symbol\", f\"Transcript_{i}\")\n",
        "\n",
        "        # Use the already filtered text from your first code block\n",
        "        filtered_text = item.get(\"filtered_text\", \"\").strip()\n",
        "\n",
        "        if not filtered_text:\n",
        "            print(f\"‚ö†Ô∏è No risk sentences found for {company}\")\n",
        "            continue\n",
        "\n",
        "        # Extract risks from the filtered text\n",
        "        risks = extract_risks_gemini_full_text(filtered_text, company, model=model)\n",
        "        all_risks.extend(risks)\n",
        "\n",
        "\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"‚úÖ Extraction complete: {len(all_risks)} total risks found across {len(subset)} transcripts.\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    return all_risks\n",
        "\n"
      ],
      "metadata": {
        "id": "alcvUGN-oNvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell executes the full Gemini extraction pipeline on the filtered transcripts. It processes each item in the subset, sends the risk-related text to the LLM, and collects all extracted risk objects into the all_risks list."
      ],
      "metadata": {
        "id": "Rs7Fggaq_VKx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_risks = process_transcripts_filtered(subset)"
      ],
      "metadata": {
        "id": "XKp8lxtloi4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this cell, we convert the extracted LLM outputs into a pandas DataFrame and remove duplicate risk entries (same company, risk type, and sentence). This produces a clean structured dataset (df_risks) that we use for all subsequent analysis and visualization."
      ],
      "metadata": {
        "id": "C4rCIIo1_dpN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_risks = pd.DataFrame(all_risks)\n",
        "\n",
        "# Remove duplicates\n",
        "df_risks.drop_duplicates(subset=[\"company\", \"risk_type\", \"sentence\"], inplace=True)\n",
        "\n",
        "print(\"Number of risks extracted:\", len(df_risks))\n",
        "df_risks.head()\n"
      ],
      "metadata": {
        "id": "RXUjM09DGVJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell computes basic descriptive statistics on the extracted risks, including how many risks each company mentions and which risk types occur most frequently. It then visualizes the top risk types using a countplot, giving an overview of the dominant themes in the dataset."
      ],
      "metadata": {
        "id": "kPSPzF8E_o9r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code takes the structured risk data extracted from company transcripts by the LLM pipeline and adds a semantic analysis layer using embeddings and clustering.\n",
        "\n",
        "Generate Embeddings\n",
        "The SentenceTransformer converts each sentence into a numerical vector\n",
        "Cluster the Embeddings (KMeans)\n",
        "The algorithm groups semantically similar sentences together into clusters\n",
        "Dimensionality Reduction (UMAP)\n",
        "UMAP reduces them to two dimensions\n",
        "\n",
        "Cluster Summaries\n",
        "The code prints a few example sentences from each cluster, giving you a quick qualitative sense of what each semantic group represents (e.g., financial risk, supply chain issues, cybersecurity)."
      ],
      "metadata": {
        "id": "_-o-tgFuPAJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Semantic Embeddings + Clustering + Visualization ---\n",
        "# (Run this AFTER your LLM extraction pipeline)\n",
        "\n",
        "!pip install sentence-transformers umap-learn scikit-learn plotly\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import KMeans\n",
        "import umap\n",
        "import plotly.express as px\n",
        "\n",
        "# --- Step 1: Convert structured risk data to text form ---\n",
        "# You can adjust what you embed (sentence, company, risk_type, etc.)\n",
        "texts = [\n",
        "    f\"{r.get('company','UNKNOWN')} - {r.get('risk_type','unspecified')} - {r.get('sentence','')}\"\n",
        "    for r in all_risks\n",
        "    if r.get('sentence')\n",
        "]\n",
        "\n",
        "if not texts:\n",
        "    raise ValueError(\"No risk sentences found in all_risks. Ensure your pipeline extracted data before this step.\")\n",
        "\n",
        "# --- Step 2: Generate embeddings ---\n",
        "print(\"üîç Generating embeddings...\")\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")  # small, efficient transformer model\n",
        "embeddings = model.encode(texts, show_progress_bar=True)\n",
        "\n",
        "# --- Step 3: Semantic clustering ---\n",
        "n_clusters = min(10, len(texts) // 3 or 1)  # adaptive number of clusters\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "cluster_labels = kmeans.fit_predict(embeddings)\n",
        "\n",
        "# --- Step 4: Dimensionality reduction for visualization ---\n",
        "reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, random_state=42)\n",
        "embedding_2d = reducer.fit_transform(embeddings)\n",
        "\n",
        "# --- Step 5: Create dataframe for visualization ---\n",
        "df_vis = pd.DataFrame({\n",
        "    \"x\": embedding_2d[:, 0],\n",
        "    \"y\": embedding_2d[:, 1],\n",
        "    \"cluster\": cluster_labels,\n",
        "    \"text\": texts,\n",
        "})\n",
        "\n",
        "# --- Step 6: Interactive visualization ---\n",
        "fig = px.scatter(\n",
        "    df_vis,\n",
        "    x=\"x\",\n",
        "    y=\"y\",\n",
        "    color=df_vis[\"cluster\"].astype(str),\n",
        "    hover_data={\"text\": True, \"x\": False, \"y\": False, \"cluster\": True},\n",
        "    title=\"Semantic Clustering of Extracted Risk Sentences\",\n",
        "    width=900,\n",
        "    height=600\n",
        ")\n",
        "fig.update_traces(marker=dict(size=7, opacity=0.8))\n",
        "fig.show()\n",
        "\n",
        "# --- Step 7 (optional): Cluster summary ---\n",
        "print(\"\\nüß† Cluster summaries:\")\n",
        "for c in sorted(df_vis['cluster'].unique()):\n",
        "    sample_texts = df_vis[df_vis['cluster'] == c]['text'].head(3).tolist()\n",
        "    print(f\"\\nCluster {c} ({len(df_vis[df_vis['cluster'] == c])} items):\")\n",
        "    for t in sample_texts:\n",
        "        print(f\"  ‚Ä¢ {t[:120]}...\")\n"
      ],
      "metadata": {
        "id": "dJrA6wGpOZ7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualization (Plotly Scatter Plot)\n",
        "Each point in the scatter plot represents one extracted risk sentence:\n",
        "\n",
        "Position (x, y) ‚Üí semantic similarity (closer points = more similar meaning)\n",
        "Color ‚Üí cluster membership (grouped by KMeans)\n",
        "\n",
        "The interactive scatter plot reveals semantic relationships among all extracted risk sentences.\n",
        "* Tightly grouped clusters indicate that the LLM identified similar risks across multiple companies.\n",
        "* Widely spread clusters suggest a diverse set of risk discussions.\n",
        "* Hovering over points lets you inspect exact sentences and see which companies discussed similar risk topics.\n",
        "* The number and density of clusters provide a quick visual summary of how varied or concentrated the risk landscape is across your sample."
      ],
      "metadata": {
        "id": "OaEuZrT0OvVQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this cell, we construct a bipartite network where companies and risk types are represented as nodes. An edge is added whenever a company mentions a given risk type in its earnings transcript. We restrict the graph to the top 25 most frequent risk types to keep the visualization readable.\n",
        "\n",
        "Node colors:\n",
        "\n",
        "üü¢ Green nodes represent companies\n",
        "\n",
        "üîµ Blue nodes represent risk types\n",
        "\n",
        "A spring layout is used to position the nodes, making clusters and strong associations easier to interpret."
      ],
      "metadata": {
        "id": "NmRTkV7R_5_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get top 25 risk types\n",
        "top_25_risks = df_risks['risk_type'].value_counts().head(25).index\n",
        "\n",
        "# Filter dataframe to only include top 25 risks\n",
        "df_top25 = df_risks[df_risks['risk_type'].isin(top_25_risks)]\n",
        "\n",
        "# Build network graph\n",
        "G = nx.Graph()\n",
        "for _, row in df_top25.iterrows():\n",
        "    G.add_node(row['company'], type='company')\n",
        "    G.add_node(row['risk_type'], type='risk')\n",
        "    G.add_edge(row['company'], row['risk_type'], sentence=row['sentence'])\n",
        "\n",
        "# Visualize\n",
        "plt.figure(figsize=(14,10))\n",
        "pos = nx.spring_layout(G, seed=42)\n",
        "node_colors = ['lightgreen' if G.nodes[n].get('type')=='company' else 'skyblue' for n in G.nodes()]\n",
        "nx.draw(G, pos, with_labels=True, node_color=node_colors, node_size=1500, font_size=10)\n",
        "plt.title(\"Company-Risk Network (Top 25 Risk Types)\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "GrYFIOnnBgfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation**\n",
        "\n",
        "The network shows small, disconnected clusters because each company mentions only a few risks in the filtered transcripts. Green nodes are companies and blue nodes are risk types. Each mini-cluster represents one company and the specific risks it discussed (e.g., REGN focuses on competitive and financial risks, CPAY on operational risks, VLO/CTAS on financial + operational). Because the sample is small, there are no shared risks forming larger connected groups."
      ],
      "metadata": {
        "id": "Tf_YeehFAVhh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell performs basic network analysis on the company‚Äìrisk graph:\n",
        "\n",
        "* Degree centrality: Identifies the most connected nodes (companies or risks that appear most frequently).\n",
        "\n",
        "* Betweenness centrality: Finds nodes that act as bridges between others (important connectors in the network).\n",
        "\n",
        "* Community detection: Uses modularity to group companies and risks into meaningful clusters based on shared connections.\n",
        "\n",
        "This helps identify which risks are most central, which companies discuss the most diverse risks, and whether any natural communities form in the dataset."
      ],
      "metadata": {
        "id": "F_AvXJNDAeCE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Degree centrality\n",
        "deg_centrality = nx.degree_centrality(G)\n",
        "top_nodes = sorted(deg_centrality.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "print(\"Top nodes by degree centrality:\", top_nodes)\n",
        "\n",
        "# Betweenness centrality\n",
        "btw_centrality = nx.betweenness_centrality(G)\n",
        "top_btw = sorted(btw_centrality.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "print(\"Top nodes by betweenness centrality:\", top_btw)\n",
        "\n",
        "# Community detection (optional)\n",
        "from networkx.algorithms.community import greedy_modularity_communities\n",
        "communities = list(greedy_modularity_communities(G))\n",
        "for i, comm in enumerate(communities):\n",
        "    print(f\"Community {i+1}: {comm}\")\n"
      ],
      "metadata": {
        "id": "YcF6w3VyBiaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell attaches year and company metadata to each extracted risk.\n",
        "\n",
        "* It builds a small table with the symbol, company name, and year from the original transcript subset.\n",
        "\n",
        "* It merges this metadata into df_risks, so each risk inherits the correct year.\n",
        "\n",
        "* If duplicate merge columns appear (e.g., year_x / year_y), they are cleaned into a single year column.\n",
        "This prepares the dataset for time-based analysis (e.g., comparing risks before/after certain years)."
      ],
      "metadata": {
        "id": "6G6vm5FsAybQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build meta (ticker -> year, company_name) from your existing subset list\n",
        "meta_rows = []\n",
        "for it in subset:\n",
        "    meta_rows.append({\n",
        "        \"company\": it.get(\"symbol\"),      # you used 'symbol' as company in extraction\n",
        "        \"year\": it.get(\"year\"),\n",
        "        \"company_name\": it.get(\"company_name\")\n",
        "    })\n",
        "meta_df = pd.DataFrame(meta_rows).drop_duplicates()\n",
        "\n",
        "# Merge into df_risks\n",
        "df_risks = df_risks.merge(meta_df, on=\"company\", how=\"left\")\n",
        "\n",
        "# If duplicates from an earlier merge exist, collapse to a single 'year'\n",
        "if \"year_x\" in df_risks.columns and \"year_y\" in df_risks.columns:\n",
        "    df_risks[\"year\"] = df_risks[\"year_y\"].fillna(df_risks[\"year_x\"])\n",
        "    df_risks.drop(columns=[\"year_x\",\"year_y\"], inplace=True)\n",
        "\n",
        "print(\"Columns now:\", df_risks.columns.tolist())\n",
        "print(\"Years present:\", sorted(df_risks[\"year\"].dropna().unique().tolist()) if \"year\" in df_risks else \"no year\")\n"
      ],
      "metadata": {
        "id": "hQxiPqUt9NtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell standardizes the extracted risk types to make the analysis more consistent.\n",
        "\n",
        "* It normalizes risk labels (lowercasing, trimming).\n",
        "\n",
        "* It assigns each risk to a broader category (e.g., market/financial, cybersecurity, regulatory/legal) using simple keyword rules.\n",
        "\n",
        "* This makes it easier to group, compare, and visualize risks across companies."
      ],
      "metadata": {
        "id": "j57B6pHdA8bH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize risk types (lowercase, trim) and map synonyms to buckets\n",
        "df_risks[\"risk_type_norm\"] = (\n",
        "    df_risks[\"risk_type\"]\n",
        "      .astype(str)\n",
        "      .str.strip()\n",
        "      .str.lower()\n",
        ")\n",
        "\n",
        "# Simple keyword-based category\n",
        "def risk_category(rt: str) -> str:\n",
        "    if any(k in rt for k in [\"regulator\", \"compliance\", \"antitrust\", \"legal\"]):\n",
        "        return \"regulatory/legal\"\n",
        "    if any(k in rt for k in [\"market\", \"demand\", \"pricing\", \"macroecon\", \"inflation\", \"volatility\", \"currency\"]):\n",
        "        return \"market/financial\"\n",
        "    if any(k in rt for k in [\"supply\", \"logistic\", \"production\", \"manufactur\"]):\n",
        "        return \"supply/operations\"\n",
        "    if any(k in rt for k in [\"cyber\", \"security\", \"ransom\", \"breach\", \"attack\"]):\n",
        "        return \"cybersecurity\"\n",
        "    if any(k in rt for k in [\"reputation\", \"brand\", \"sentiment\"]):\n",
        "        return \"reputation\"\n",
        "    if any(k in rt for k in [\"health\", \"environment\", \"pollut\", \"toxic\", \"disease\", \"outbreak\"]):\n",
        "        return \"health/environment\"\n",
        "    return \"other\"\n",
        "\n",
        "df_risks[\"risk_category\"] = df_risks[\"risk_type_norm\"].apply(risk_category)\n",
        "\n",
        "# Quick check\n",
        "print(df_risks[[\"company\",\"risk_type\",\"risk_type_norm\",\"risk_category\"]].head(8))"
      ],
      "metadata": {
        "id": "gz8F5pkq9OPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell creates descriptive visualizations of the extracted risks.\n",
        "\n",
        "* It shows the most frequent normalized risk types in a horizontal bar chart.\n",
        "\n",
        "* It also plots the distribution of broader risk categories (e.g., regulatory, market, cybersecurity).\n",
        "These plots help us understand which types of risks dominate the transcripts."
      ],
      "metadata": {
        "id": "O9a5Q-8PBE1G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Counts by normalized type and by category\n",
        "top_types = df_risks[\"risk_type_norm\"].value_counts().head(15)\n",
        "top_cats  = df_risks[\"risk_category\"].value_counts()\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10,5))\n",
        "top_types.plot(kind=\"barh\", ax=ax)\n",
        "ax.set_title(\"Top risk types (normalized)\")\n",
        "ax.set_xlabel(\"Count\"); ax.set_ylabel(\"Risk type\")\n",
        "plt.tight_layout(); plt.show()\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(7,4))\n",
        "top_cats.plot(kind=\"bar\", ax=ax)\n",
        "ax.set_title(\"Risk categories\")\n",
        "ax.set_xlabel(\"Category\"); ax.set_ylabel(\"Count\")\n",
        "plt.tight_layout(); plt.show()"
      ],
      "metadata": {
        "id": "X5oPl0fi9QeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell performs a manual quality check of the LLM extraction. It randomly selects 3 extracted risks and prints the company, year, risk type, risk category, and the exact sentence the model returned. This helps evaluate whether the LLM‚Äôs outputs are accurate and meaningful."
      ],
      "metadata": {
        "id": "h6KzT39EBPkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üßê Manual inspection: 3 random extractions\")\n",
        "sample = df_risks.sample(min(3, len(df_risks)), random_state=7)\n",
        "for _, r in sample.iterrows():\n",
        "    print(\"\\n‚Äî\"*32)\n",
        "    print(\"Ticker:\", r[\"company\"], \"| Year:\", r.get(\"year\"))\n",
        "    print(\"Risk type:\", r[\"risk_type\"])\n",
        "    print(\"Category:\", r[\"risk_category\"])\n",
        "    print(\"Extracted sentence:\", r[\"sentence\"][:400], \"...\")"
      ],
      "metadata": {
        "id": "zUgg1S1D9S1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell builds a weighted bipartite network where edges represent how many times a company mentions a specific normalized risk type.\n",
        "\n",
        "* It creates an edge list with weights (frequency of co-occurrence).\n",
        "\n",
        "* It constructs separate node tables for companies and risks, including attributes like company name, year, and risk category.\n",
        "\n",
        "* It then builds the weighted graph Gw and saves all node and edge tables as CSV files for documentation and reproducibility."
      ],
      "metadata": {
        "id": "LOvKEZHiBWUY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Weighted edgelist (company ‚Äî risk_type_norm)\n",
        "edges = (\n",
        "    df_risks.groupby([\"company\",\"risk_type_norm\"], as_index=False)\n",
        "            .size()\n",
        "            .rename(columns={\"size\":\"weight\"})\n",
        ")\n",
        "# Nodes tables\n",
        "companies = df_risks[[\"company\",\"company_name\",\"year\"]].drop_duplicates().rename(columns={\"company\":\"node\"})\n",
        "risks     = df_risks[[\"risk_type_norm\",\"risk_category\"]].drop_duplicates().rename(columns={\"risk_type_norm\":\"node\"})\n",
        "\n",
        "# Build graph with weights + attributes\n",
        "Gw = nx.Graph()\n",
        "for _, row in companies.iterrows():\n",
        "    Gw.add_node(row[\"node\"], type=\"company\", company_name=row.get(\"company_name\"))\n",
        "for _, row in risks.iterrows():\n",
        "    Gw.add_node(row[\"node\"], type=\"risk\", risk_category=row.get(\"risk_category\"))\n",
        "\n",
        "for _, row in edges.iterrows():\n",
        "    Gw.add_edge(row[\"company\"], row[\"risk_type_norm\"], weight=row[\"weight\"])\n",
        "\n",
        "print(f\"Weighted graph: {Gw.number_of_nodes()} nodes, {Gw.number_of_edges()} edges\")\n",
        "\n",
        "# Export CSVs for deliverables\n",
        "edges.to_csv(\"edgelist_company_risk_weighted.csv\", index=False)\n",
        "companies.to_csv(\"nodes_companies.csv\", index=False)\n",
        "risks.to_csv(\"nodes_risks.csv\", index=False)\n",
        "print(\"Saved: edgelist_company_risk_weighted.csv, nodes_companies.csv, nodes_risks.csv\")"
      ],
      "metadata": {
        "id": "7cUQEelk9WJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell visualizes the weighted company‚Äìrisk network built earlier.\n",
        "\n",
        "* Green nodes represent companies, blue nodes represent risk types.\n",
        "\n",
        "* Edge thickness increases with the weight (how often a company mentions a risk).\n",
        "\n",
        "* The spring layout highlights which risks are most strongly associated with which companies.\n",
        "It also runs a community detection algorithm to identify clusters of companies and risks that are closely related."
      ],
      "metadata": {
        "id": "tpd6Xg0cBhzR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(14,10))\n",
        "pos = nx.spring_layout(Gw, seed=42, k=0.6)\n",
        "\n",
        "node_colors = [\"#81C784\" if Gw.nodes[n][\"type\"]==\"company\" else \"#64B5F6\" for n in Gw.nodes()]\n",
        "edge_ws = [0.6 + 0.6*Gw[u][v][\"weight\"] for u,v in Gw.edges()]  # thicker if more co-mentions\n",
        "\n",
        "nx.draw_networkx_nodes(Gw, pos, node_color=node_colors, node_size=1200)\n",
        "nx.draw_networkx_labels(Gw, pos, font_size=9)\n",
        "nx.draw_networkx_edges(Gw, pos, width=edge_ws, alpha=0.6)\n",
        "plt.title(\"Company‚ÄìRisk bipartite network (weighted)\")\n",
        "plt.axis(\"off\"); plt.tight_layout(); plt.show()\n",
        "\n",
        "# Communities on bipartite graph (greedy)\n",
        "from networkx.algorithms.community import greedy_modularity_communities\n",
        "comms = list(greedy_modularity_communities(Gw))\n",
        "print(f\"Detected {len(comms)} communities.\")\n"
      ],
      "metadata": {
        "id": "5V9hTDQ59gk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell creates a company‚Äìcompany projection of the bipartite network.\n",
        "\n",
        "* Two companies are connected if they share one or more risk types.\n",
        "\n",
        "* Edge weights represent how many risks they have in common.\n",
        "\n",
        "* It prints the most strongly connected companies and visualizes the network, showing which companies have similar risk profiles.\n",
        "This helps identify firms that face similar types of risks or operate in comparable environments."
      ],
      "metadata": {
        "id": "7kPuomgkBsov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from networkx.algorithms import bipartite\n",
        "\n",
        "companies_only = [n for n,d in Gw.nodes(data=True) if d[\"type\"]==\"company\"]\n",
        "risks_only     = [n for n,d in Gw.nodes(data=True) if d[\"type\"]==\"risk\"]\n",
        "\n",
        "# Weighted projection: edge weight = number of shared risks\n",
        "Cc = bipartite.weighted_projected_graph(Gw, companies_only)\n",
        "\n",
        "print(f\"Company‚ÄìCompany projection: {Cc.number_of_nodes()} nodes, {Cc.number_of_edges()} edges\")\n",
        "\n",
        "# Top neighbours by shared risks\n",
        "deg = sorted(Cc.degree(weight=\"weight\"), key=lambda x: x[1], reverse=True)[:10]\n",
        "print(\"Top companies by shared-risk connectivity:\", deg)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12,8))\n",
        "pos = nx.spring_layout(Cc, seed=7)\n",
        "ew = [0.8 + 0.8*Cc[u][v][\"weight\"] for u,v in Cc.edges()]\n",
        "nx.draw(Cc, pos, with_labels=True, node_color=\"#81C784\", node_size=1200, width=ew, alpha=0.8)\n",
        "plt.title(\"Company‚ÄìCompany network (shared risk types)\")\n",
        "plt.axis(\"off\"); plt.tight_layout(); plt.show()\n"
      ],
      "metadata": {
        "id": "8n6j3jA49kDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if \"year\" in df_risks.columns and df_risks[\"year\"].notna().any():\n",
        "    df_time = df_risks.dropna(subset=[\"year\"]).copy()\n",
        "    df_time[\"year\"] = df_time[\"year\"].astype(int)\n",
        "\n",
        "    # Before vs after 2022\n",
        "    before = df_time[df_time[\"year\"] < 2022]\n",
        "    after  = df_time[df_time[\"year\"] >= 2022]\n",
        "\n",
        "    top_b = before[\"risk_type_norm\"].value_counts().head(10)\n",
        "    top_a = after[\"risk_type_norm\"].value_counts().head(10)\n",
        "\n",
        "    comp = pd.DataFrame({\"Before 2022\": top_b, \"2022 and later\": top_a}).fillna(0).astype(int)\n",
        "    display(comp)\n",
        "\n",
        "    ax = comp.plot(kind=\"bar\", figsize=(12,6), width=0.8, color=[\"#7FC97F\",\"#BEAED4\"])\n",
        "    ax.set_title(\"Top Risk Types: Before vs After 2022\")\n",
        "    ax.set_xlabel(\"Risk Type\"); ax.set_ylabel(\"Count\")\n",
        "    plt.xticks(rotation=45, ha=\"right\"); plt.tight_layout(); plt.show()\n",
        "\n",
        "    # Trend over time\n",
        "    yearly_counts = df_time.groupby(\"year\")[\"risk_type_norm\"].count()\n",
        "    plt.figure(figsize=(10,4))\n",
        "    sns.lineplot(x=yearly_counts.index, y=yearly_counts.values, marker=\"o\")\n",
        "    plt.title(\"Number of risk mentions per year\"); plt.xlabel(\"Year\"); plt.ylabel(\"Count\")\n",
        "    plt.grid(True, alpha=0.3); plt.tight_layout(); plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No 'year' available for time analysis.\")\n"
      ],
      "metadata": {
        "id": "eMq2KKp59nUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown\n",
        "Markdown(\"\"\"\n",
        "### Limitations (LLM Extraction)\n",
        "- Some **hallucinated** risk labels (over-general categories).\n",
        "- **Boundary errors**: occasionally merges two sentences or misses implicit risks.\n",
        "- **Schema sensitivity**: JSON sometimes inconsistent ‚Äî handled by cleaning.\n",
        "- **Subset size**: runtime constraints limit coverage; results illustrate method rather than population truth.\n",
        "\n",
        "**Mitigations:** pre-filtering with spaCy to reduce tokens; normalized risk types; used weighted edges and projections; documented manual inspection.\n",
        "\"\"\")\n"
      ],
      "metadata": {
        "id": "9jqd2ai_9o41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_risks.to_csv(\"risks_extracted.csv\", index=False)\n",
        "plt.savefig(\"last_figure.png\")  # call right after your last plot if you want the image\n",
        "print(\"Saved risks_extracted.csv and last_figure.png\")\n"
      ],
      "metadata": {
        "id": "oEQtRWPv9tHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell saves the extracted risks to a CSV file (risks_extracted.csv) and exports the final visualization as an image file (last_figure.png). These files can be included in the project deliverables or uploaded to GitHub for documentation and reproducibility."
      ],
      "metadata": {
        "id": "9JKh24dAEuVR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze > requirements.txt\n"
      ],
      "metadata": {
        "id": "1AxyD6vTqfmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('requirements.txt')\n",
        "files.download('risks_extracted.csv')\n",
        "files.download('last_figure.png')\n"
      ],
      "metadata": {
        "id": "d2WVwcz9rAoa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}